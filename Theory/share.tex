\documentclass{article}
\usepackage{listings}                                           %插入代码
\usepackage{geometry}                                           %设置页面大小边距等
\usepackage{graphicx}                                           %插入图片
\usepackage{amssymb}                                            %为了用\mathbb
\usepackage{amsmath}                                            %数学方程的显示
\usepackage{listings}                                           %插入代码
\usepackage{fancyhdr}                                           %设置页眉页脚
\usepackage{lastpage}                                           %总页数
\usepackage{hyperref}                                           %引用网页
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{subcaption} 
\usepackage{mathrsfs}



\geometry{a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm}        %一定要放在前面！
\pagestyle{fancy}                                               %设置页眉页脚
\lhead{Guanyu Chen}                                       %页眉左Fir                                        

\rhead{Diffusion Model}                                                %章节信息
\cfoot{\thepage/\pageref{LastPage}}                             %当前页，记得调用前文提到的宏包
\lfoot{Zhejiang University}
\rfoot{College Of Integrated Circuits}
\renewcommand{\headrulewidth}{0.1mm}                            %页眉线宽，设为0可以去页眉线
\renewcommand{\footrulewidth}{0.1mm}                            %页脚线宽，设为0可以去页眉线
\setlength{\headwidth}{\textwidth}

\hypersetup{                                                    %设置网页链接颜色等
    colorlinks=true,                                            %链接将会有颜色，默认是红色
    linkcolor=blue,                                             %内部链接，那些由交叉引用生成的链接将会变为蓝色（blue）
    filecolor=magenta,                                          %链接到本地文件的链接将会变为洋红色（magenta）
    urlcolor=blue,                                              %链接到网站的链接将会变为蓝绿色（cyan）
}

\lstset{  
    basicstyle=\ttfamily,  
    keywordstyle=\color{blue},  
    language=Python,  
    numbers=left,  
    numberstyle=\tiny\color{gray},  
    frame=single,  
    breaklines=true  
}  

\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\newtheorem{solution}{Solution:}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}{Algorithm}
\newtheorem{lemma}{Lemma}

\title{Theoretical Part of Diffusion Model}
\author{Guanyu Chen}
\date{\today}
\begin{document}
\maketitle
\tableofcontents
\newpage
Here is the theoretical part of Diffusion Model. 
\section{Langevin SDE}
The Langevin SDE has the following form:
\begin{equation}
    X_{t + s} = X_t + \nabla \log p_t(x_t)s + \sqrt{2s}\xi
\end{equation}
where $X_t\in \mathcal{R}^d, p_t(x_t)=p(X_t=x_t)$, $\xi\sim N(0, I)$, $I$ is identical matrix of $m \times m$. Our goal is to sample from specific $p(x, t)$.

\begin{theorem}
    The density of Langevin Diffusion Model converges to $p(x)$ over time. In other words, if $X_t\sim p(x)$, then $X_{t+s}\sim p(x)$ for $\forall s>0$. 
\end{theorem}
\begin{proof}
    Let  $\mu_{t}(f)=E\left[f\left(X_{t}\right)\right]$. Consider  $\mu_{t+\tau}(f)=E\left[f\left(X_{t+\tau}\right)\right]$, as $\tau \rightarrow 0$. Then  
\begin{equation}
    \begin{aligned}
        \mu_{t+\tau}=&E\left[f\left(X_{t}+\nabla \log p_t\left(x_{t}\right) \cdot \tau+\sqrt{2 \tau} \xi\right)\right]\\
        =&E\left[f\left(x_{t}\right)+\nabla^{\top} f\left(x_{t}\right)\left(\tau \nabla \log p_t\left(x_{t}\right)+\sqrt{2 \tau} \xi\right)\right. \\
        &+\frac{1}{2}\left.\left(\nabla^{\top}\log p_t(x_t)\tau + \sqrt{2\tau}\xi\right)\nabla^2f(x_t)\nabla\log p_t(x_t)\tau + \sqrt{2\tau}\xi\right]\\
        =&E\left[f\left(x_{t}\right)\right]+E\left[\tau \nabla^{\top}f\left(x_{t}\right) \nabla \log p_t\left(x_{t}\right)\right]\\
        &+\frac{\tau^{2}}{2} E\left[\nabla^{\top} \log p\left(x_{t}\right) \cdot \nabla^{2} f\left(x_{t}\right) \cdot \nabla \log p\left(x_{t}\right)\right] +E\left[\tau \xi^{\top} \nabla^{2} f\left(x_{t}\right) \xi\right]
    \end{aligned}
\end{equation}
 
The second term:
\begin{equation}
    \begin{aligned}
        &\tau E\left[\nabla^{\top} f \nabla \log p_{t}\right] \\
        =&\tau \int \nabla f \cdot \nabla \log p_{t} p_{t} d x=\tau \int \nabla f \cdot \nabla p_{t} d x \\
        =&-\tau \int \operatorname{tr}\left(\nabla^{2} f\right) \cdot p_{t} d x=-\tau E\left[\operatorname{tr}\left(\nabla^{2} f\right)\right]\\
        =&-\tau E\left[\xi^{\top} \nabla^{2} f \xi\right] \\
    \end{aligned}
\end{equation}
Then 
\begin{equation}
    \mu_{t+\tau} =E\left[\frac{1}{2} \nabla^{\top} \log p_{t} \nabla^{2} f \nabla \log p_{t}\right] \cdot \tau^{2}=O\left(\tau^{2}\right)
\end{equation}
Hence we have $\frac{d}{dt}(\mu_t)=0$, i.e. $E[\mu_t]=E[\mu_{t+s}]$ for $\forall s>0$.
\end{proof}

\begin{remark}
    We define the density of normal distribution $N(x ; \mu, \Sigma)$, and its log-density, gradient of density and score as follows:
    \begin{equation}\left\{
        \begin{aligned}
            &N(x ; \mu, \Sigma)=\frac{1}{\sqrt{(2 \pi)^{d}|\Sigma|}} e^{-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1}(x-\mu)}\\
            &\log N(x ; \mu, \Sigma)=-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1}(x-\mu)-\log \left(\sqrt{(2 \pi)^{d}|\Sigma|}\right) . \\
            &\nabla_{x} N(x ; \mu, \Sigma)=N(x ; \mu, \Sigma)\Sigma^{-1}(x-\mu) \\
            &\nabla_{x} \log N(x ; \mu, \Sigma)=-\Sigma^{-1}(x-\mu) .
        \end{aligned}\right.
    \end{equation}
\end{remark}

Actually, Langevin SDE is not necessary be as above i.e. the diffusion term is not necessary to be $\sqrt{2}$. The reason is to guarantee the stationary distribution of $p_t(x)$.
i.e. the term $\frac{\partial p(x,t)}{\partial t}=0$ in FPK equation. If the diffusion term is $g(t)$, then by FPK equation, we have 
$$\nabla_x\cdot(fp-\frac{1}{2}g^2(t)\nabla p)=0$$
then $f(x,t) = \frac{1}{2}g^2(t)\frac{\nabla_x p(x, t)}{p(x,t)}=\frac{1}{2}g^2(t)\nabla_x\log p(x, t)$.

\section{Linear SDE}
Then we consider linear SDE having the form:
\begin{equation}
    dX_t = (a(t)X_t + b(t))dt + g(t)dW_t
\end{equation}
where $X_t\in \mathcal{R}^d, W_t\in \mathcal{R}^m$ with diffusion factor $Q\in \mathcal{R}^{m\times m}$, then $a(t)\in \mathcal{R}^{d\times d}, b(t)\in \mathcal{R}^d, g(t)\in \mathcal{R}^{d\times m}$. 
By Euler Maruyama method, it can be approximated By
\begin{equation}
    \begin{aligned}
        X_{t+s}&=X_t + (a(t)X_t + b(t))s+g(t)\sqrt{sQ}\xi\\
        &=(1+a(t)s)X_t + b(t)s + g(t)\sqrt{sQ}\xi
    \end{aligned}
\end{equation}
where $\xi\sim N(0, I_m)$. Usually we need to consider the expectation, variance and distribution of $X_t$. But the stochastic value of $X_t$ is dependent of $x_0$. Then first we consider
\begin{equation}
    \begin{aligned}
    E\left[X_{t+s} | X_{0}\right]-E\left[X_{t} | X_{0}\right] & \approx\left(a(t) E\left[X_{t} | X_{t}\right]+b(t)\right) s+g(t) \sqrt{sQ} E[\xi] \\
    & =\left(a(t) E\left[X_{t} | X_{0}\right]+b(t)\right) s .
    \end{aligned}
\end{equation}


Note  $e(t)=E\left[X_{t} | X_{0}\right]$, then
\begin{equation}
    e^{\prime}(t)=\lim _{s \rightarrow 0} \frac{E\left[X_{t+s} | X_{0}\right]-E\left[X_{t} | X_{0}\right]}{s}=a(t) \cdot e(t)+b(t) . \quad e(0)=X_{0} .    
\end{equation}
which is an ODE system, having solution
\begin{equation}
    e(t)=e^{\int_{0}^{t} a(s) d s}\cdot\left(X_{0}+\int_{0}^{t} e^{-\int_{0}^{s} a(r) d r} b(s) d s\right)
\end{equation}
Therefore
\begin{equation}
    \begin{aligned}
    E\left[X_{t}\right] & =E\left[E\left[X_{t} | X_{0}\right]\right]=E[e(t)] \\
    & =e^{\int_{0}^{t} a(s) d s}\cdot\left(E\left[X_{0}\right]+\int_{0}^{t} e^{-\int_{0}^{s} a(r) d r} b(s) d s\right) 
    \end{aligned}
\end{equation}

Similarly, Note $\operatorname{Var}\left(X_{0} | X_{0}\right)=v(t)$:
then $\operatorname{Var}\left(X_{t+s} | X_{0}\right)=(1+s a(t))^{2} \operatorname{Var}\left(X_{t} | X_{0}\right)+s gQg^\top$. Then
\begin{equation}
    \begin{aligned}
        V^{\prime}(t)&=\lim _{s \rightarrow 0} \frac{\operatorname{Var}\left(X_{t+s} | X_{0}\right)-\operatorname{Var}\left(X_{t} | X_{0}\right)}{s}\\
        =&\left[\left(a^{2}(t) s+2 a(t)\right) v(t)+g^{2}(t)\right]|_{s \rightarrow 0}\\ 
        =&2 \alpha(t) V(t)+g(t)Qg^\top(t), \qquad V(0)=0
    \end{aligned}
\end{equation}
Solution is:
\begin{equation}
    v(t)=e^{\int_{0}^{t} 2a(s) d s}\cdot\left(\int_{0}^{t} e^{-\int_{0}^{s} 2 a(r) d r} g(s)Qg^\top(s) d s\right)
\end{equation} 
By law of total variance:
\begin{equation}
    \begin{aligned}
        \operatorname{Var}\left(X_{t}\right)=&E\left[X_{t}^{2}\right]-E^{2}\left[X_{t}\right]=E\left[ E\left[X_{t}^{2} | X_{0}\right]\right]-E^{2}\left[X_{t}\right] \\
        =&E\left[\operatorname{Var}\left(X_{t} | X_{0}\right)+E^{2}\left[X_{t} | X_{0}\right]\right]-E^{2}\left[X_{t}\right] \\
        =&E\left[\operatorname{Var}\left(X_{t} | X_{0}\right)\right]+E\left[E^{2}\left[X_{t} | X_0\right]\right]-E^{2}\left[E\left[X_{t} | X_{0}\right]\right]\\
        =&E\left[\operatorname{Var}\left(X_{t} | X_{0}\right)\right]+\operatorname{Var}\left(E\left[X_{t} | X_0\right)\right)
    \end{aligned}
\end{equation}
then 
\begin{equation}
    \begin{aligned}
        \operatorname{Var}(X_t)=&E[V(t)]+\operatorname{Var}(e(t))\\
        =&e^{\int_{0}^{t} 2 a(s) d s}\cdot \left(\int_{0}^{t} e^{-\int_{0}^{s} 2 a(r) d r} g(s)Qg^\top(s) d s\right) +e^{\int_{0}^{t} 2 a(s) d s} \cdot \operatorname{Var}\left(X_{0}\right) .
    \end{aligned}
\end{equation}

We have the following theorem which is crucial for diffusion models. Usually, we assume $Q=I_m$.
\begin{theorem}\label{thm1}
    If  $X_{t+s}=(1+a(t) s) X_{t}+b(t) s+g(t) \sqrt{s} \xi$\\
    then $X_{t} | X_{0} \sim N\left(E\left[X_{t} | X_{0}\right], \operatorname{Var}\left(X_{t} | X_{0}\right)\right)$, 
    where $E\left[X_{t} | X_{0}\right]=e(t), \operatorname{Var}\left(X_{t} | X_{0}\right)=V(t)$.        
\end{theorem}

It should be noted that $e(t)$ is related to $X_0$ and t, while $V(t)$ only depends on $t$!

Next, we will see how the above formula can be applied to diffusion modtels. There are three frameworks to build SDEs for diffusion models, VP, VE and sub-VP.
\begin{definition}
    Noise function  $\beta(t)$ . s.t. $\beta(0)=0 ; \beta^{\prime}(t) \geqslant 0 ; \beta(t) \rightarrow \infty \text { as } t \rightarrow \infty$
\end{definition}

\subsection{Variance Preserving (VP) SDE}
So if we have diffusion model like:
\begin{equation}
\begin{aligned}
    X_{t_{i+1}}&=\sqrt{1-\left(\beta\left(t_{i+1}\right)-\beta\left(t_{i}\right)\right)}X_{t_i}+\sqrt{\left(\beta\left(t_{i+1}\right)-\beta\left(t_{i}\right)\right)}\xi\\
    &=\sqrt{1-\Delta\beta(t_i)}X_{t_i}+\sqrt{\Delta \beta(t_i)}\xi
\end{aligned}
\end{equation}
Then the conditional distribution is given by:
\begin{equation}
    q\left(X_{t_{i+1}} | X_{t_{i}}\right)=N(x_{t_{i+1}} ; \sqrt{1-\Delta \beta\left(t_{i}\right)}X_{t_i}, \Delta \beta\left(t_{i}\right))
\end{equation}
Then we need to estimate  $\theta$  drift term  $f$  and diffusion term  $g$:

\begin{equation}
    \begin{aligned}
        f(x, t)&=\lim _{h \rightarrow 0} \frac{E\left[X_{t+h}-X_{t} | X_{t}=x\right]}{h} \\
            &=\lim _{h \rightarrow 0} \frac{x \sqrt{1-\Delta \beta(t)}-x}{h}=-\frac{x}{2} \beta^{\prime}(t) . \\
    g(t) &= \sqrt{\lim _{h \rightarrow 0} \frac{N\left[X_{t+h} | X_{t}=x\right]}{h}}=\sqrt{\lim _{h \rightarrow 0} \frac{\beta(t+h)-\beta(t)}{h}}=\sqrt{\beta^{\prime}(t)}
    \end{aligned}
\end{equation}
Then the model can be written as
$d x=-\frac{x}{2} \beta^{\prime}(t) d t+\sqrt{\beta^{\prime}(t)} d W_{t}$


Then by Theorem \ref{thm1} we have
\begin{equation}
    \left\{\begin{aligned}
    &E\left[X_{t} | X_{0}\right]=X_{0} e^{\int_{0}^{t}-\frac{1}{2} \beta'(s) d s}=X_{0} e^{-\frac{1}{2} \beta(t)} \\
    &E\left[X_{t}\right]=E\left[X_{0}\right] e^{-\frac{1}{2} \beta(t)} \\
    &V\left(X_{t} | X_{0}\right)=\int_{0}^{t} e^{\int_{0}^{s} \beta^{\prime}(r) d r} \beta^{\prime}(s) d s \cdot e^{-\beta(t)}=1-e^{-\beta(t)} \\
    &V\left(X_{t}\right)=1-e^{-\beta(t)}+V\left(X_{0}\right) e^{-\beta(t)}=1+\left(V\left(X_{0}\right)-1\right) e^{-\beta(t)} .
    \end{aligned}\right.
\end{equation}
So as  $t \rightarrow \infty,\beta(t) \rightarrow \infty$, then  $E \rightarrow 0, V \rightarrow 1$, i.e. 
$X_{t} | X_{0} \sim N\left(E\left[X_{t} | X_{0}\right], \operatorname{Var}\left|X_{t}\right| X_{0}\right)\rightarrow N(0,1) \text{ as } t \rightarrow \infty$.
 
\subsection{Variance-Exploding SDE}
Here is the model: 
$X_{t+h}=X_{t}+\sqrt{\Delta \beta(t)} \xi$

Similarly we can compute the $f(x, t)\equiv 0$ and $g(t)=\sqrt{\beta(t)}$.
Hence  
\begin{equation}\left\{
    \begin{aligned}
        &E\left[X_{0} | X_{0}\right]=X_{0}\\
        &E\left[X_{t}\right]=E\left[X_{0}\right] \\ 
        &V\left(X_{t} | X_{0}\right)=\int_{0}^{t} e^{\int_{0}^{s} 0 d r} \beta^{\prime}(s) d s=\beta(t)\\ 
        &V\left(X_{t}\right)=V\left[X_{0}\right]+\beta(t)
    \end{aligned}\right.
\end{equation}

So the expectation value is constant and the variance is increasing monotonical. \\
If we rescale  $X_{t}$ as $Y_{t}=\frac{X_{t}}{\sqrt{\beta(t)}}$, then $Y_t \rightarrow N(0,1), t \rightarrow \infty$.

\subsection{Sub-VP SPE}
Here, we set the dift and diffusion term as
\begin{equation}
\begin{aligned}
        &f(x, t)=-\frac{1}{2} \beta^{\prime}(t) \\
        &g(t)=\sqrt{\beta^{\prime}(t)\left(1-e^{-2 \beta(t)}\right)}
\end{aligned}
\end{equation}
As the same, we can compute that.
\begin{equation}
    \left\{\begin{aligned}
    &E\left[X_{t} | X_{0}\right]=X_{0} e^{-\frac{1}{2} \beta(t)} \\
    &E\left[X_{t}\right]=E\left[X_{0}\right] e^{-\frac{1}{2} \beta(t)} \\
    &V\left(X_{t} | X_{0}\right)=\left(1-e^{-\beta(t)}\right)^{2} \\
    &V\left(X_{t}\right)=\left(1-e^{-\beta(t)}\right)^{2}+V\left(X_{t}\right) e^{-\beta(t)} .
    \end{aligned}\right.
\end{equation}

We can find out that the variance is always smaller thar VP SDE.

\begin{remark}
    To sum up, finally we hope that $X_t$ converges to a normal distribution by choosing different drift and diffusion functions. 
    For generative model, the goal is to sample from a Data distribution $p_{data}$. We have known that if we set the initial distribution $p_0(x_0)=p(X_0=x_0)\sim p_{data}$, 
    then after $t=T$, the distribution of $X_t$ is tend to be $N(0, 1)$ under certain conditions. 
    
    So the idea is backward: if we sample from $X_T\sim N(0, 1)$, and then run SDE backwards, could we get the initial distribution?
\end{remark}

\section{Reverse SDE}
Assume we have forward SDE: from $X_0\sim p_0,X_T\sim p_T$,
\begin{equation}\label{forward}
    dX_t = f(X_t, t)dt + g(t)dW_t
\end{equation}
Then we define the reverse SDE as: from $X_T\sim p_T$,
\begin{equation}
    d\bar{X_t}=\bar{f}(\bar{X}_t, t)dt + \bar{g}(t)d\bar{W_t}
\end{equation}
where $\bar{W}_t$ is Brownian Motion runns backward in time, i.e. $\bar{W}_{t-s}-\bar{W}_t$ is independent of $\bar{W}_t$. We can approximate by EM:
\begin{equation}
    \bar{X}_{t-s}-\bar{X}_t=-s\bar{f}(\bar{X}_t, t) + \sqrt{s}\bar{g}(t)\xi
\end{equation}
So the problem is: If given $f,g$, are there $\bar{f},\bar{g}$ s.t. the reverse time diffusion process $\bar{X}_t$ has the same distribution as the forward process $X_t$? Yes!
\begin{theorem}
    The reverse SDE with $\bar{f},\bar{g}$ having the following form has the same distribution as the forward SDE \ref{forward}:
    \begin{equation}\left\{
        \begin{aligned}
            &\bar{f}(x,t)=f(x,t)-g^2(x,t)\frac{\partial}{\partial x}\log p_t(x)\\
            &\bar{g}=g(t)
        \end{aligned}\right.
    \end{equation}
    i.e. 
    \begin{equation}
        d\bar{X}_t = \left[f(\bar{X}_t, t)-g^2(t)\log p_t(x_t)\right]dt+g(t)d\bar{W}_t
    \end{equation}
\end{theorem}
\begin{proof}
    The proof is skipped.
\end{proof}
This theroem allows us to learn how to generate samples from $p_{data}$.
\begin{algorithm}:\\
Step1. Select $f(x, t)$ and $g(t)$ with affine drift coefficients s.t. $X_T\sim N(0, 1)$\\
Step2. Train a network $s_\theta(x, t)=\frac{\partial}{\partial x}\log p_t(x)$ where $p_t(x)=p(X_t=x)$ is the forward distribution.\\
Step3. Sample $X_T$ from $N(0, 1)$, then run reverse SDE from T to 0:
\begin{equation}
    \bar{X}_{t-s} = \bar{X}_t + s\left[g^2(t)s_\theta(\bar{X}_t, t)-f(\bar{X}_t, t)\right] + \sqrt{s}g(t)\xi
\end{equation}
\end{algorithm}

\section{Denoising Diffusion}
\subsection{Loss function}
Normally we can define the loss function as follows:
\begin{equation}
    \begin{aligned}
        L_\theta & = \frac{1}{T}\int_0^T\lambda(t)\underset{x_0\sim p_{data}}{E}\left[\underset{x_t\sim p_{t|0}(x_t|x_0)}{E}\left[\|s_\theta(x_t, t)-\nabla_{x_t}\log p_t(x_t)\|^2\right]\right]dt\\
        &=\underset{t\sim U(0, T)}{E}\left[\lambda(t)\underset{x_0\sim p_{data}}{E}\left[\underset{x_t\sim p_{t|0}(x_t|x_0)}{E}\left[\|s_\theta(x_t, t)-\nabla_{x_t}\log p_t(x_t)\|^2\right]\right]\right]
    \end{aligned}
\end{equation}
It should be clearified that $p_{t|0}(x_t|x_0)=p(X_t=x_t|X_0=x_0)$. So $$p_t(x_t)=\int p_{t|0}(x_t|x_0)p_0(x_0)dx_0=E_{x_0\sim p_{data}}\left[p_{t|0}(x_t|x_0)\right]$$
where $p_{t}(x)=p\left(X_{t}=x\right)$, $p_{t | 0}(x | y)=p\left(X_{t}=x | X_{0}=y\right)$. Then

\begin{equation}
    \begin{aligned}
    \nabla \log p_{t}(x) & =\frac{1}{p_{t}(x)} \nabla p_{t}(x) . \\
    & =\frac{1}{p_{t}(x)} \nabla \int p_{t | 0}(x | y) p_{0}(y) d y \\
    & =\frac{1}{p_{t}(x)} \int \nabla p_{t | 0}(x | y) p_{0}(y) d y \\
    & =\frac{1}{p_{t}(x)} \int \frac{\nabla p_{t | 0}(x | y)}{p_{t | 0}(x | y)} p_{0}(y) \cdot p_{t | 0}(x | y) d y \\
    & =\int \nabla_{x} \log \left(p_{t | 0}(x | y)\right) \cdot p_{0 | t}(y | x) d y \\
    & =\underset{y\sim p_{0|t}(y|x)}{E}\left[\nabla_{x} \log \left(p_{t | 0}(x | y)\right)\right]
    \end{aligned}
\end{equation}

Where we have used the following lemma:
\begin{lemma}
    \begin{equation}
        \underset{x_0\sim p_0}{E}\left[\underset{x_t\sim p_{t|0}(\cdot|x_0)}{E}\left[\underset{x'_0\sim p_{0|t}(\cdot|x_t)}{E}\left[f(x_t, x'_0)\right]\right]\right]=\underset{x_0\sim p_0}{E}\left[\underset{x_t\sim p_{t|0}(\cdot|x_0)}{E}\left[f(x_t, x_0)\right]\right]
    \end{equation}
\end{lemma}
\begin{proof}
    Easy to prove.
\end{proof}
Then we can rewrite the loss function as:
\begin{equation}
    \begin{aligned}
        L_{\theta}&=\underset{t\sim U(0,T)}{E}\left[\lambda(t) \underset{x_{0}\sim p_{data}}{E}\left[\underset{x_{t}\sim p_{t|0}(x_t|x_0)}{E}\left[\left\|S_{\theta}\left(x_{t}, t\right)-\nabla _{x_t}\log p_{t}\left(x_{t}\right)\right\|^{2}\right]\right.\right.\\ 
        &\leqslant \underset{t\sim U(0,T)}{E}\left[\lambda(t) \underset{x_{0}\sim p_{data}}{E}\left[\underset{x_{t}\sim p_{t|0}(x_t|x_0)}{E}\left[\underset{y\sim p_{data}}{E}\left[\left\|S_{\theta}\left(x_{t}, t\right)-\nabla_{x_{t}} \log \left(p_{t|0}(x_t | y)\right)\right\|^{2}\right]\right]\right]\right] \\
        &=\underset{t\sim U(0, T)}{E}\left[\lambda(t) \underset{x_{0}\sim p_{data}}{E}\left[\underset{x_{t}\sim p_{t|0}(x_t|x_0)}{E}\left[\| S_{\theta}\left(x_{t}, t\right)-\nabla_{x_{t}} \log \left(p_{t|0}\left(x_{t} | x_{0}\right) \|^{2}\right]\right]\right]\right.
    \end{aligned}
\end{equation}


Since  $p_{t|0}\left(x_{t} | x_{0}\right)=p\left(X_{t}=x_{t} | X_{0}=x_{0}\right)$  has been discussed:

$$p_{t | 0}\left(x_{t} | x_{0}\right) \sim N\left(x_{t} ; E\left[X_{t}=x_t | X_{0}=x_{0}\right], \operatorname{Var}\left(X_{t}=x_t | X_{0}=x_{0}\right)\right) .$$

Then by theorem \ref{thm1}, x can be written as $x=e(t, X_0)+\sqrt{V(t)}\xi$, where $\xi\sim N(0, 1)$, then the score function is:
\begin{equation}
    \frac{\partial}{\partial x} \log p_{t | 0}\left(x | x_{0}\right)=-\frac{x-E_{t | 0}\left[x | x_{0}\right]}{\operatorname{Var}_{t | 0}\left(x | x_{0}\right)}=-\frac{x-e(t, X_0)}{V(t)}\sim -N\left(0, \frac{1}{V(t)}\right)
\end{equation}
So
\begin{equation}
\begin{aligned}
    L_\theta=&\underset{t\sim U(0,T)}{E}\left[\lambda (t)\underset{x_0\sim p_{data}}{E}\left[\underset{\xi\sim N(0, 1)}{E}\left[\left\|s_\theta\left(\sqrt{V(t)}\xi+e(t, X_0), t\right) + \frac{\xi}{\sqrt{V(t)}}\right\|^2\right]\right]\right]\\
    =&\underset{t\sim U(0,T)}{E}\left[\lambda (t)\underset{x_0\sim p_{data}}{E}\left[\frac{1}{V(t)}\underset{\xi\sim N(0, 1)}{E}\left[\left\|\xi_\theta\left(\sqrt{V(t)}\xi+e(t, X_0), t\right)-\xi\right\|^2\right]\right]\right]
\end{aligned}    
\end{equation}
where $\xi_\theta = -\sqrt{V(t)}s_\theta$ is called denoising network.

\subsection{With Classifier Guidance}
Though we can produce pictures by sampling from normal distribution, we still cannot control what we will generate. What we want to do is something like:
"Give me the pictures of number 6", then the model can sample from the normal distribution and do the denoising to generate pics of 6. 

Usually, we can do something like: train a model for every class label. This do make the model smaller, but increases number of models. 
Think about it, when the label is TEXT, it is impossiable to train a model for each sentences. 

So, the initial distribution is $p_0(x|y)$ given the label y. Similarly, we will convert the data distribution $p_{data}(x|y)$ to final distribution, normal distribution expected.
Then we SDE becomes: $X_t \sim p_t(x|y)$


\begin{equation}
    \begin{aligned}
    & p_{t}(x \mid y)=p\left(X_{t}=x \mid y\right)=\frac{p\left(y \mid X_{t}=x\right) p\left(X_{t}=x\right)}{p(y)} \\
    \Rightarrow & \log \left(p_{t}(x \mid y)\right)=\log \left(p\left(y \mid X_{t}=x\right)\right)+\log \left(p\left(X_{t}=x\right)\right)-\log (p(y)) \\
    \Rightarrow & \nabla_{x} \log \left(p_{t}(x \mid y)\right)=\nabla_{x} \log \left(p\left(y \mid X_{t}=x\right)\right)+\nabla_{x} \log \left(p\left(X_{t}=x\right)\right)
    \end{aligned}
\end{equation}
    
    
We have finished training  $\nabla_{x} \log \left(p\left(X_{t}=x\right)\right)$  in sampling. Then we need to estimate  $\nabla_{x} \log \left(p\left(y \mid X_{t}=x\right)\right)$. This is the conditional protability, we end up with a sharp factor s: $p^{\prime}\left(y \mid X_{t}=x\right)$ , then:
\begin{equation}
    \nabla_{x} \log \left(p_{t}(x \mid y)\right)=S \nabla_{x} \log \left(p\left(y \mid x_{t}=x\right)\right)+\nabla_{x} \log \left(p\left(x_{t}=x\right)\right)
\end{equation}
Note  $\omega_{\theta}(y \mid x, t)$  to learn  $s \nabla_{x} \log \left(p\left(y \mid X_{t}=x\right)\right)$ 

\subsection{Classifier Guidance Free}
\begin{equation}
    \begin{aligned}
        &\gamma \nabla_{x} \log \left(p\left(y \mid X_{t}=x\right)\right)\\
         =& \gamma\left(\nabla_x \log \left(p(X_t=x|y)\right) - \nabla_x \log \left(p_t(x)\right)\right)
    \end{aligned}
\end{equation}
Then 
\begin{equation}
    \begin{aligned}
        &\nabla_{x} \log_\gamma \left(p_{t}(x \mid y)\right)\\
        =&(1-\gamma)\nabla_x \log \left(p_t(x)\right) + \gamma\nabla_x \log \left(p(X_t=x|y)\right)
    \end{aligned}
\end{equation}
Hence we only need one conditional denoising network, and using null condition to represent the unconditional model.


\section{Flow Matching}
\subsection{FPK Equation}
We have discussed the FPK Equation in 'learnsde'. 
\begin{theorem}[Fokken-Planck-Kolmogorov equation]
    The density function $p(x, t)$ of $X_t$ s.t. 
    \begin{equation}
        dX_t = f(X_t, t)dt + G(X_t,t)dW_t
    \end{equation}
    solves the PDE:
    \begin{equation}
        \frac{\partial p(x, t)}{\partial t}=-\sum_{i} \frac{\partial}{\partial x_{i}}\left[f_{i}(x, t) p(x, t)\right]+\frac{1}{2} \sum_{i, j} \frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\left[\left(G Q G^{\top}\right)_{i j} p(x, t)\right]
    \end{equation}
    The PDE is called FPK equation / forwand Kolmogorov equation.
\end{theorem}
It can be rewritten as:
\begin{equation}
    \begin{aligned}
        \frac{\partial p(x, t)}{\partial t} &= -\nabla\cdot\left[f(x, t) p(x, t)\right]+\frac{1}{2} \nabla^2\cdot\left[\left(G Q G^{\top}\right) p(x, t)\right] \\
        &=-\nabla\cdot\left[f(x, t) p(x, t)-\frac{1}{2} \nabla\cdot\left[\left(G Q G^{\top}\right) p(x, t)\right]\right]
    \end{aligned}
\end{equation}
Here, if we only consider $G(X_t, t)=g(t)$, then we notice that $M=GQG^T$ is independent of $X_t$, so we can write:
\begin{equation}
    \begin{aligned}
        \frac{\partial p(x, t)}{\partial t} &= -\nabla\cdot\left(fp-\frac{1}{2}\nabla\cdot (Mp)\right)\\
        &=-\nabla\cdot\left(fp-\frac{1}{2}M\nabla p\right)\\
        &=-\nabla\cdot\left[\left(f-\frac{1}{2}M\frac{\nabla p}{p}\right)p\right]\\
        &=-\nabla\cdot\left[\left(f-\frac{1}{2}M\nabla\log p\right)p\right]
    \end{aligned}
\end{equation}
So we find out that if we have an ODE s.t. $dZ_t=F(Z_t, t)dt$ with $Z_0 \sim p_0$, instead of a SDE, then by FPK equation, the density $p(z, t)$ satisfies:
\begin{equation}
    \frac{\partial p(z, t)}{\partial t}=-\nabla\cdot\left(F(z, t)p(z, t)\right)
\end{equation}
So if we set $F(z, t)=f(z, t) - \frac{1}{2}M(t)\nabla\log p(z, t)$, then $p(z, t)$ is exactly like the density $p(x, t)$ of $X_t$ in SDE. 
So theoretically, we can do the diffusion like reverse ode!

This is the topic discussed in 'Probability Flow'.
\subsection{Probability Flow}
Define a flow $\phi: \mathcal{R}^d\times [0, 1]\rightarrow \mathcal{R}^d$ is a flow generated by a vector field $v: \mathcal{R}^d \times [0, 1]\rightarrow \mathcal{R}^d$ i.e.
\begin{equation}\left\{
    \begin{aligned}
        \frac{\partial \phi(x, t)}{\partial t} &= v(\phi(x, t), t)\\
        \phi(0, x)&=x
    \end{aligned}\right.
\end{equation}
The flow means that under the vector field $v$, if the initial point is $x$, then the flow push the point after time $t$ to $\phi(x, t)$. That is $\phi$ gives the evolution trajectory of $x$ under the vector field $v$.
So normally, we can consider the flow $\phi(x, t)$ as $X_t$ in SDE:
\begin{equation}
    dX_t = v(X_t, t)dt + 0dW_t
\end{equation}
with $X_0=x$. It turns out that it is actually an ODE, a SDE without diffusion term. Similar to SDE, if $X_0=x\sim p_0(x)$, we have the probability density $p(x, t)$ satisfies FPK equation:
\begin{equation}
    \frac{\partial p(x, t)}{\partial t}=-\nabla\cdot\left(v(x, t)p(x, t)\right)
\end{equation}
which is a special case of FPK equation, called \textbf{Continuity Equation}. So, typically, we can solution to an ODE is a flow.

\begin{theorem}
    \begin{equation}
        v_t(x)=v(x, t) = \int v_{t1}(x|x_1)\frac{p_{t1}(x|x_1)p_1(x_1)}{p_t(x)}dx_1
    \end{equation}
    where $p_{t1}(x|x_1)=p(X_t=x|X_1=x_1), p_1(x_1)=p(X_1=x_1)$, and $v_{t|1}(x|x_1)$ is the vector field generating $p_{t1}(x|x_1)$.
\end{theorem}
\begin{proof}
    By $p_t(x) = \int p_{t1}(x|x_1)p_1(x_1)dx_1$, we have:
    \begin{equation}
        \begin{aligned}
            \frac{\partial p_t(x)}{\partial t} &= \int \frac{\partial p_{t1}(x|x_1)}{\partial t}p_1(x_1)dx_1\\
            &= \int \left(-\nabla\cdot\left(v_{t1}(x|x_1)p_{t1}(x|x_1)p_1(x_1)\right)\right)dx_1\\
            &= -\nabla\cdot\left(\int v_{t1}(x|x_1)p_{t1}(x|x_1)p_1(x_1)dx_1\cdot p_t(x)\right)\\
            &= -\nabla\cdot\left(v_t(x)p_t(x)\right)
        \end{aligned}
    \end{equation}
then 
\begin{equation}
    \begin{aligned}
        &\int v_{t1}(x|x_1)p_{t1}(x|x_1)p_1(x_1)dx_1=v_t(x)p_t(x)\\
        \Rightarrow &v_t(x) = \int v_{t1}(x|x_1)\frac{p_{t1}(x|x_1)p_1(x_1)}{p_t(x)}dx_1
    \end{aligned}
\end{equation}
\end{proof}

So the objective of Flow Matching Model can be described as: Let $p_t(x)$ be the density with initial $p_0(x)$, which is designed to be a simple distribution, like normal distribution. 
So let $p_1(x)$ be the approximation equal in distribution to$ p_{data}$. Then we need to design a flow to match the flow s.t. $p_1$ can properly approximate $p_{data}$.


\section{What is Diffusion after all?}
Diffusion is a phenomenon that occurs broadly in nature. 
It is can be described in many different ways with different applications. 
It is all known that the diffusion term is about the Laplacian operator. But why? 
Here, I want to discuss the diffusion from the following aspects:
\begin{itemize}
    \item FPK Equation(SDEs)
    \item Flow Map(O/PDEs)
    \item Applications in VLSI
\end{itemize}
\subsection{From FPK Equation}
At the beginning, the diffusion phenomenon is observed through the motion of particles(Brownian motion). 
Normally, the SDE can be written as:
\begin{equation}
    dX_t = f(X_t, t)dt + G(X_t, t)dW_t
\end{equation}
Here, we skip the drift term $f(X_t, t)$ and only consider the diffusion term $G(X_t, t)dW_t$, i.e.
\begin{equation}
    dX_t = G(X_t, t)dW_t
\end{equation}
Then by FPK equation, we can derive 
\begin{theorem}
    The probability density function $p(x, t)$ satisfies:
\begin{equation}
    \frac{\partial p(x, t)}{\partial t} = \frac{1}{2} \sum_{i, j} \frac{\partial^{2}}{\partial x_{i} \partial x_{j}}\left[\left(G Q G^{\top}\right)_{i j} p(x, t)\right]=\frac{1}{2}\nabla \cdot \left(\nabla\cdot (GQG^Tp(x, t))\right)
\end{equation}
Specially, when $G(X_t, t)=G(t)$ and $Q=I$, we have:
\begin{equation}
    \frac{\partial p}{\partial t} = \nabla \cdot \left(\frac{GG^T}{2}\nabla p\right)
\end{equation}
\end{theorem}
So, when $X_0\sim p_0$, we can then compute the diffusion density $p(x, t)$ by solving the FPK equation.
\subsection{From Flow Map}
Since we have the definition of \textbf{Flow Map} $\phi_s^t(\mathbf{x})$, which is controlled by vector field $V(\phi_s^t(\mathbf{x}), t)$, 
then just think the $\phi_0^t(\mathbf{x})$ as the trajectory of the particle beginning at $x$ over time, noted as $\phi_t(x)$.
Then the vector field is actually the velocity field of the particle, so we have:
\begin{equation}\left\{
    \begin{aligned}
        \frac{\partial \phi_t(\mathbf{x})}{\partial t} &= V(\phi_t(\mathbf{x}), t)\\
        \phi_0(\mathbf{x}) &= \mathbf{x}
    \end{aligned}\right.
\end{equation}
The motion of particles described by $\phi_t$ determines how the density $p_t(x)$ evolves over time. 
\begin{theorem}
    When the initial density $p_0(x)$ is known, the density field can be expressed as:
\begin{equation}
    p(\phi_t(x), t) = \frac{p_0(x)}{\left|\det J_{\phi_t}(x)\right|}
\end{equation}
\end{theorem}

It should be noted that $\phi_t(x)$ is actually the same as $X_t$ in SDE, then similarly, the density is:
\begin{equation}
    \phi_t(x) \sim p_t(x)
\end{equation}
So, the flow map is an ODE, which is a special case of SDE without diffusion term. Then we have:
\begin{theorem}[Continuity Equation]
    The probability density function $p(x, t)$ of $X_t$ satisfies:
    \begin{equation}
        \frac{\partial p(x, t)}{\partial t} = -\nabla\cdot\left(V(x, t)p(x, t)\right)
    \end{equation}
    which is called \textbf{Continuity Equation}.
\end{theorem}
\begin{remark}
    The continuity equation can also be derived from the Conservation of Mass. 
\end{remark}

\begin{theorem}
    When the incompressible condition is satisfied, that is $\nabla\cdot V=0$, then the flow $\phi_t(x)$ is \textbf{measure preserving}, that is:
    \begin{equation}
        \left|\det J_{\phi_t}(x)\right|=1, \text{i.e.}p(\phi_t(x), t) = p_0(x)
    \end{equation}
\end{theorem}

\begin{definition}[Flux]
    We find that $-V(x, t)p(x, t)$ is actually the flux $\mathcal{F}(x, t)$ of the particle.
\end{definition}
Then the continuity equation can be rewritten as:
\begin{equation}
    \frac{\partial p(x, t)}{\partial t} = \nabla\cdot\left(\mathcal{F}(x, t)\right)
\end{equation}
Then we find that if the flux s.t. $F = \frac{1}{2}\nabla\cdot\left(GQG^Tp(x, t)\right)$, then $p(x, t)$ describes the diffusion process. This is the famous Fick's Law.
\begin{theorem}[Fick's Law]
    Fick's Law describes the relationship between the flux $\mathcal{F}(x, t)$ of the particle and the concentration/density $p(x, t)$.:
    \begin{equation}
        \mathcal{F}(x, t) = \frac{1}{2}\nabla\cdot\left(GQG^Tp(x, t)\right)
    \end{equation}
    Specifically, when $G(X_t, t)=G(t)$ and $Q=I$, we have:
    \begin{equation}
        \mathcal{F}(x, t) = \frac{GG^T}{2}\nabla p(x, t)
    \end{equation}
    Then 
    \begin{equation}
        \frac{\partial p(x, t)}{\partial t} = \nabla \cdot\left(\frac{GG^T}{2}\nabla p(x, t)\right)
    \end{equation}
\end{theorem}
\subsection{Solution}
Note $-\frac{GG^T}{2}$ is actually the diffusion coefficient $\mathcal{D}$. Then we have the diffusion equation:
\begin{equation}
    \frac{\partial p(x, t)}{\partial t} = \nabla \cdot\left(\mathcal{D}\nabla p(x, t)\right)
\end{equation}
with initial condition $p(x, 0) = p_0(x)$. We can use the Fourier Transform to solve this equation. 
\begin{theorem}
    The solution to the diffusion equation is:
    \begin{equation}
        \begin{aligned}
            p(x, t) &= \mathscr{F}^{-1}\left[\tilde{p}_0(\lambda)\exp\left(-\lambda^T\mathcal{D}\lambda t\right)\right]=\left(p_0\star \mathcal{G}_{2t\mathcal{D}}\right)(x)\\
            & = \frac{1}{\sqrt{(4\pi t)^d\det(\mathcal{D})}}\int_{\mathcal{R}^d}\left(p_0(\xi)\exp\left(-\frac{1}{4t}\left(x-\xi\right)^T\mathcal{D}^{-1}\left(x-\xi\right)\right)\right)d\xi
        \end{aligned}
    \end{equation}
    where $\tilde{p}_0(\lambda) = \mathscr{F}(p_0(x))$ is the Fourier Transform of $p_0(x)$. $\mathcal{G}_{2t\mathcal{D}}$ is the Gaussian Kernel with variance $2t\mathcal{D}$.
\end{theorem}
\begin{proof}
    First, assume the Fourier Transform of $p(x, t)$ is $\tilde{p}(x, t)$:
    \begin{equation}\left\{
        \begin{aligned}
            \tilde{p}(x, t) &= \mathscr{F}\left[p(x, t)\right]=\int_{\mathcal{R}^d} p(x, t)e^{-i\lambda\cdot x}dx\\
            p(x, t) &= \mathscr{F}^{-1}\left[\tilde{p}(x, t)\right]=\frac{1}{(2\pi)^d}\int_{\mathcal{R}^d} \tilde{p}(x, t)e^{i\lambda\cdot x}dx
        \end{aligned}\right.
    \end{equation} 
    Then, we have:
    \begin{equation}\left\{
        \begin{aligned}
            \mathscr{F}\left[\nabla\cdot \mathbf{v}\right] &= i\lambda\cdot \mathscr{F}\left[\mathbf{v}\right]\\
            \mathscr{F}\left[\mathcal{D}\nabla p\right] &= i\mathcal{D}\lambda\mathscr{F}\left[p\right]
        \end{aligned}\right.
    \end{equation}
    Then, 
    \begin{equation}
        \begin{aligned}
            &\mathscr{F}\left[\frac{\partial p}{\partial t}\right] = \frac{d}{dt}\mathscr{F}\left[p\right] = \mathscr{F}\left[\nabla\cdot\left(\mathcal{D}\nabla p\right)\right] \\
            =& i\lambda\cdot \mathscr{F}\left[\mathcal{D}\nabla p\right]=-\lambda^T\mathcal{D}\lambda \mathscr{F}\left[p\right]
        \end{aligned}
    \end{equation}
    where $\lambda = \left(\lambda_1, \lambda_2, \cdots, \lambda_d\right)^T$. 

    Therefore, $\mathscr{F}\left[p\right] = \tilde{p_0}\exp\left(-\lambda^T\mathcal{D}\lambda t\right)$. 
    Since $\mathscr{F}\left[N(x|0, 2t\mathcal{D})\right]=\exp\left(-\lambda^T\mathcal{D}\lambda t\right)$, which gives the theorem.
\end{proof}

\begin{remark}
    Specially, 1. When the initial density $p_0(x)$ is $\delta(x - x_0)$, the solution is:
\begin{equation}
    p(x, t) = \frac{1}{\sqrt{(4\pi t)^d\det{\mathcal{D}}}}\exp\left(-\frac{(x-x_0)^T\mathcal{D}^{-1}(x-x_0)}{4t}\right)\sim N(x_0, 2t\mathcal{D})
\end{equation}
2. When the initial density $p_0(x)$ is a Gaussian distribution $N(\mu, \Sigma)$, the solution is:
\begin{equation}
    p(x, t) = \frac{1}{\sqrt{(2\pi)^d\det(\Sigma + 2t\mathcal{D})}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\left(\Sigma + 2t\mathcal{D}\right)^{-1}\left(x-\mu\right)\right)\sim N(\mu, \Sigma + 2t\mathcal{D})
\end{equation}
(The Fourier transform of $\left(\mu, \Sigma \right)$ is $\exp \left(-i\lambda^T\mu + \frac{1}{2}\lambda^T\Sigma\lambda\right)$.)
\end{remark}

Till here, we can see the insight of diffusion. It is actually a process of smoothing the initial density by the Gaussian Kernel.

\subsection{From Applications in VLSI}
...

\section{OT}
\end{document} 